{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bonus1.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "ZOxkT8uRYrrN",
    "uxQPaFFZYrEr",
    "lkvcpZA7mLm5",
    "Lj41VW9VpamL",
    "BlzXfuhvhF0L",
    "REtpu0AnhSq5",
    "ukKiTN7Uhj-_"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## for data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## for processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for model \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "##Other\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import KeyedVectors\n",
    "from typing import List, Tuple\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score\n",
    "import seqeval\n",
    "\n",
    "from utility import StudDataset\n",
    "from utility import CRF\n",
    "from utility import scatter_sum\n",
    "from utility import StudentParams"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTiqzWuHj-qU",
    "outputId": "542989b7-390a-4053-9800-7631494f16c8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def seed_all(seed: int = 42):\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "id": "YOw5Na5dlm3o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "seed_all()"
   ],
   "metadata": {
    "id": "H0UNjqW7lrEW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1b542be6-a42f-410f-a5e6-0319d0a1a6ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  42  ]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "temp_path = \"../../\"\n",
    "data_path = os.path.join(temp_path, \"data\")\n",
    "model_p = os.path.join(temp_path, \"model\")\n",
    "train_path = os.path.join(data_path, \"train.tsv\")\n",
    "dev_path = os.path.join(data_path, \"dev.tsv\")"
   ],
   "metadata": {
    "id": "TzTsem6OltUT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def read_dataset(path: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    tokens_s = []\n",
    "    labels_s = []\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith(\"#\\t\"):\n",
    "                tokens = []\n",
    "                labels = []\n",
    "            elif line == \"\":\n",
    "                tokens_s.append(tokens)\n",
    "                labels_s.append(labels)\n",
    "            else:\n",
    "                token, label = line.split(\"\\t\")\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "\n",
    "    assert len(tokens_s) == len(labels_s)\n",
    "\n",
    "    return tokens_s, labels_s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "\tdef __init__(self, params: StudentParams):\n",
    "\t\tsuper(StudentModel, self).__init__()\n",
    "\t\tself.params = params\n",
    "\t\tself.device = params.device\n",
    "\n",
    "\t\t# EMBEDDING LAYERS\n",
    "\t\tself.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.params.word_vocab.vectors), freeze=self.params.words_freeze, padding_idx=self.params.word_vocab.key_to_index[\"<PAD>\"])\n",
    "\t\tself.pos_embedding = nn.Embedding(num_embeddings=self.params.pos_vocab_size, embedding_dim=self.params.pos_embedding_dim, padding_idx=self.params.pos_vocab_index[\"<PAD>\"])\n",
    "\t\tself.char_embedding = nn.Embedding(num_embeddings=self.params.char_vocab_size, embedding_dim=self.params.char_embedding_dim, padding_idx=self.params.char_vocab_index[\"<PAD>\"])\n",
    "\n",
    "\t\t# LAYER 1.1: Char BiLSTM\n",
    "\t\tself.char_lstm = nn.LSTM(input_size=self.params.char_embedding_dim, hidden_size=self.params.char_hidden_dim, num_layers=self.params.char_lstm_layers, bidirectional=self.params.char_bidir, dropout=self.params.char_dropout if self.params.char_lstm_layers > 1 else 0, batch_first=True)\n",
    "\t\tself.dense_char_embedding = nn.Linear(self.params.char_hidden_dim * (2 if self.params.char_bidir else 1), self.params.char_hidden_dim * (2 if self.params.char_bidir else 1))\n",
    "\t\tself.char_dropout = nn.Dropout(self.params.char_dropout)\n",
    "\n",
    "\t\t# LAYER 1.2: Global BiLSTM\n",
    "\t\tself.global_contextual_embedding = nn.LSTM(input_size=self.params.global_embedding_dim, hidden_size=self.params.global_hidden_dim, num_layers=self.params.global_lstm_layers, bidirectional=self.params.global_bidir, dropout=self.params.global_dropout if self.params.global_lstm_layers > 1 else 0, batch_first=True)\n",
    "\n",
    "\t\tself.dense_global_embedding = nn.Linear(self.params.global_hidden_dim * (2 if self.params.global_bidir else 1), self.params.global_hidden_dim * (2 if self.params.global_bidir else 1))\n",
    "\t\tself.global_dropout = nn.Dropout(self.params.global_dropout)\n",
    "\n",
    "\t\t# LAYER 2: Feature BiLSTM\n",
    "\t\tself.combined_lstm = nn.LSTM(input_size=self.params.full_embedding_dim, hidden_size=self.params.combined_hidden_dim, num_layers=self.params.combined_lstm_layers, bidirectional=self.params.combined_bidir, dropout=self.params.combined_dropout if self.params.combined_lstm_layers > 1 else 0, batch_first=True)\n",
    "\n",
    "\t\tself.dense_combined_embedding = nn.Linear(self.params.combined_hidden_dim * (2 if self.params.combined_bidir else 1), self.params.feature_dim_out)\n",
    "\t\tself.combined_dropout = nn.Dropout(self.params.combined_dropout)\n",
    "\n",
    "\t\t# LAYER 3.1: Classificator\n",
    "\t\tself.batchnorm = nn.BatchNorm1d(self.params.feature_dim_out)\n",
    "\t\tself.SELU = nn.SELU()\n",
    "\n",
    "\t\tself.fc = nn.Linear(self.params.feature_dim_out, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, self.params.num_classes)\n",
    "\t\tself.fc_dropout = nn.Dropout(self.params.classificator_dropout)\n",
    "\n",
    "\t\t# LAYER 3.2: Classificator\n",
    "\t\tself.conll_fc = nn.Linear(self.params.feature_dim_out, self.params.conll_num_classes)\n",
    "\t\tself.conll_fc_dropout = nn.Dropout(self.params.conll_classificator_dropout)\n",
    "\n",
    "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\t\t# LAYER 4\n",
    "\t\tif self.params.crf:\n",
    "\t\t\tself.crf = CRF(self.params.num_classes).to(self.device)\n",
    "\t\t\t# LAYER 4.2\n",
    "\t\t\tself.conll_crf = CRF(self.params.conll_num_classes).to(self.device)\n",
    "\n",
    "\tdef ner_loss(self, y_pred, y, mask, criterion=None, conll=False):\n",
    "\t\tif criterion is None:\n",
    "\t\t\tcriterion = nn.CrossEntropyLoss(ignore_index=StudDataset.encode_class(\"<PAD>\"), reduction=\"mean\")\n",
    "\n",
    "\t\tif self.params.crf:\n",
    "\t\t\tif conll:\n",
    "\t\t\t\tloss = -(self.conll_crf(y_pred, y, mask)).mean()\n",
    "\t\t\telse:\n",
    "\t\t\t\tloss = -(self.crf(y_pred, y, mask)).mean()\n",
    "\t\telse:\n",
    "\t\t\t# labels  [[1,2,3], [18, 12, 3]] after the view(-1) [1,2,3, 18, 12, 3]\n",
    "\t\t\ty_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "\t\t\ty = y.view(-1)\n",
    "\t\t\t# FLATTENED MASK\n",
    "\t\t\tf_mask = mask.view(-1)\n",
    "\n",
    "\t\t\t# FILTER NOT PADDING\n",
    "\t\t\ty_pred = y_pred[f_mask]\n",
    "\t\t\ty = y[f_mask]\n",
    "\t\t\tloss = criterion(y_pred, y)\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef forward(self, x, conll=False, verbose=False):\n",
    "\n",
    "\t\t# Unpack chars and sentence_char_len\n",
    "\t\twords = x[0]\n",
    "\t\tposes = x[1]\n",
    "\t\tchars = x[2]\n",
    "\t\tchar_indexes = x[3]\n",
    "\t\tmask = x[4]\n",
    "\n",
    "\t\t# Embedding layers\n",
    "\t\tposes_out = self.pos_embedding(poses)\n",
    "\t\twords_out = self.word_embedding(words)\n",
    "\n",
    "\t\t# FIRST LSTM layer\n",
    "\t\tif self.params.char:\n",
    "\t\t\tchars_out = self.char_embedding(chars)\n",
    "\t\t\tchar_lstm_out, _ = self.char_lstm(chars_out)\n",
    "\n",
    "\t\t\t# char_lstm_out = self.dense_char_embedding(char_lstm_out)\n",
    "\t\t\tchar_lstm_out = self.char_dropout(char_lstm_out)\n",
    "\n",
    "\t\t\tif verbose: print(\"FIRST lstm out shape: {}\".format(char_lstm_out.shape))\n",
    "\t\t\t# COMBINE CHARS FOR EACH WORDS\n",
    "\n",
    "\t\t\tbatch_sentences = scatter_sum(char_lstm_out, char_indexes, dim=1)[:, :words_out.shape[1], :]\n",
    "\n",
    "\t\t\tif verbose: print(\"Scatter out shape: {}\".format(batch_sentences.shape))\n",
    "\t\t\tif verbose: print(\"words out shape: {}\".format(words_out.shape))\n",
    "\t\t\tif verbose: print(\"poses out shape: {}\".format(poses_out.shape))\n",
    "\n",
    "\t\t\tcombined_embeddings = torch.cat((words_out, batch_sentences, poses_out), 2)\n",
    "\t\telse:\n",
    "\t\t\tcombined_embeddings = torch.cat((words_out, poses_out), 2)\n",
    "\n",
    "\t\tif self.params.global_vector:\n",
    "\t\t\t# Global Contextual Embedding\n",
    "\n",
    "\t\t\tglobal_lstm_out, _ = self.global_contextual_embedding(combined_embeddings)\n",
    "\t\t\tglobal_lstm_out = self.dense_global_embedding(global_lstm_out)\n",
    "\t\t\tglobal_lstm_out = self.global_dropout(global_lstm_out)\n",
    "\n",
    "\t\t\tglobal_context_emb = torch.sum(global_lstm_out, dim=1)\n",
    "\t\t\tglobal_context_emb = torch.unsqueeze(global_context_emb, dim=1)\n",
    "\n",
    "\t\t\tglobal_context_emb = global_context_emb.expand(-1, global_lstm_out.shape[1], -1)\n",
    "\n",
    "\t\t\tif verbose: print(\"global_context_emb embeddings shape: {}\".format(global_context_emb.shape))\n",
    "\n",
    "\t\tif self.params.global_vector:\n",
    "\t\t\tcombined_embeddings = torch.cat((global_context_emb, combined_embeddings), 2)\n",
    "\n",
    "\t\tif verbose: print(\"COMBINED embeddings shape: {}\".format(combined_embeddings.shape))\n",
    "\n",
    "\t\t# SECOND LSTM LAYER\n",
    "\t\tfull_lstm_out, _ = self.combined_lstm(combined_embeddings)\n",
    "\n",
    "\t\tif verbose: print(\"SECOND lstm shape: {}\".format(full_lstm_out.shape))\n",
    "\t\tfeature_lstm_out = self.dense_combined_embedding(full_lstm_out)\n",
    "\t\tfeature_lstm_out = self.combined_dropout(feature_lstm_out)\n",
    "\n",
    "\t\tif verbose: print(\"FEATURE extractor shape: {}\".format(feature_lstm_out.shape))\n",
    "\t\tfeature_lstm_out = self.batchnorm(feature_lstm_out.permute(0, 2, 1))\n",
    "\n",
    "\t\tif verbose: print(\"BATCHNORM shape: {}\".format(feature_lstm_out.shape))\n",
    "\n",
    "\t\tactivation_function = self.SELU\n",
    "\n",
    "\t\tout = activation_function(feature_lstm_out.permute(0, 2, 1))\n",
    "\n",
    "\t\t# IF IS USED THE CONLL DATASET USE A CLASSIFICATOR WITH LESS CLASSES\n",
    "\t\tif conll:\n",
    "\t\t\tout = self.conll_fc(out)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tout = self.fc(out)\n",
    "\t\t\tout = self.fc_dropout(out)\n",
    "\t\t\tout = self.fc2(out)\n",
    "\n",
    "\t\tlogits = self.softmax(out)\n",
    "\t\tif verbose: print(\"LOGITS shape: {}\".format(out.shape))\n",
    "\n",
    "\t\tif self.params.crf:\n",
    "\t\t\tif conll:\n",
    "\t\t\t\tout = self.conll_crf.viterbi_decode(logits, mask)\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = self.crf.viterbi_decode(logits, mask)\n",
    "\t\treturn logits, out\n",
    "\n",
    "\n",
    "\tdef predict(self, tokens: List[List[str]], conll = False) -> List[List[str]]:\n",
    "\t\tall_predict = []\n",
    "\t\tdev_dataset = StudDataset(tokens, self.params.word_vocab.key_to_index, self.params.pos_vocab_index, self.params.char_vocab_index, device=self.params.device, conll_dataset=conll)\n",
    "\n",
    "\t\tdataloader = DataLoader(dev_dataset, batch_size=self.params.batch_size, collate_fn=dev_dataset.collate_fn)\n",
    "\t\tself.eval()\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor (xd, ys) in dataloader:\n",
    "\t\t\t\tmask = xd[\"mask\"].to(self.device)\n",
    "\t\t\t\tx = (xd[\"words\"].to(self.device), xd[\"poses\"].to(self.device), xd[\"chars\"].to(self.device), xd[\"scattered\"].to(self.device), mask)\n",
    "\n",
    "\t\t\t\thidden, out = self(x,conll)\n",
    "\n",
    "\t\t\t\t### START EVAL PART ###\n",
    "\t\t\t\tfor s_ine, sentence in enumerate(out):\n",
    "\t\t\t\t\tall_predict.append([StudDataset.decode_class(x, conll) for x in sentence])\n",
    "\t\t\t\t### END EVAL PART ###\n",
    "\n",
    "\t\treturn all_predict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Def Train-Eval functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "BlzXfuhvhF0L",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, criterion, device, conll=False):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for (xd, ys) in train_dataloader:\n",
    "\n",
    "        y = ys[\"labels\"].to(device)\n",
    "\n",
    "        mask = xd[\"mask\"].to(device)\n",
    "        x = (xd[\"words\"].to(device), xd[\"poses\"].to(device), xd[\"chars\"].to(device), xd[\"scattered\"].to(device), mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden, out = model(x, conll=conll)\n",
    "\n",
    "        loss = model.ner_loss(hidden, y, mask, criterion, conll=conll)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_dataloader)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device, labels_s=None):\n",
    "    all_predict = []\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (xd, ys) in iterator:\n",
    "\n",
    "            mask = xd[\"mask\"].to(device)\n",
    "            x = (xd[\"words\"].to(device), xd[\"poses\"].to(device), xd[\"chars\"].to(device), xd[\"scattered\"].to(device), mask)\n",
    "            y = ys[\"labels\"].to(device)\n",
    "\n",
    "            hidden, out = model(x)\n",
    "\n",
    "            ### START EVAL PART ###\n",
    "            for s_ine, sentence in enumerate(out):\n",
    "                all_predict.append([StudDataset.decode_class(x) for x in sentence])\n",
    "            ### END EVAL PART ###\n",
    "\n",
    "            loss = model.ner_loss(hidden, y, mask, criterion )\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if labels_s is not None:\n",
    "            acc = accuracy_score(labels_s, all_predict)\n",
    "            f1 = f1_score(labels_s, all_predict, average=\"macro\")\n",
    "\n",
    "    return epoch_loss / len(iterator),acc, f1"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zfWdnJ65hF0L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Init Params Dataloaders and Model"
   ],
   "metadata": {
    "id": "REtpu0AnhSq5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "params = StudentParams()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_t, train_l = read_dataset(train_path)\n",
    "dev_t, dev_l = read_dataset(dev_path)\n",
    "if params.additional_dataset:\n",
    "    add2_t, add2_l = read_dataset(os.path.join(data_path,\"CoNLL.tsv\"))\n",
    "\n",
    "    add_t, add_l = read_dataset(os.path.join(data_path,\"WNUT17.tsv\"))\n",
    "    add_t = [ [s.lower() for s in x] for x in add_t if len(x) > 10]\n",
    "    add_l = [ x for x in add_l if len(x) > 10]\n",
    "    train_t = train_t + add_t\n",
    "    train_l = train_l + add_l\n",
    "\n",
    "train_dataset = StudDataset(train_t, params.word_vocab.key_to_index, params.pos_vocab_index,\n",
    "                          params.char_vocab_index, lemming=params.lemming, device=params.device, labels=train_l)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params.batch_size, collate_fn=train_dataset.collate_fn,\n",
    "                              shuffle=True)\n",
    "\n",
    "dev_dataset = StudDataset(dev_t, params.word_vocab.key_to_index, params.pos_vocab_index, params.char_vocab_index,\n",
    "                        lemming=params.lemming, device=params.device, labels=dev_l)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=params.batch_size, collate_fn=dev_dataset.collate_fn)\n",
    "\n",
    "if params.additional_dataset:\n",
    "    add2_t_dataset = StudDataset(add2_t, params.word_vocab.key_to_index, params.pos_vocab_index,\n",
    "                              params.char_vocab_index, lemming=params.lemming, device=params.device ,labels=add2_l, conll_dataset=True)\n",
    "\n",
    "    add2_t_dataloader = DataLoader(add2_t_dataset, batch_size=params.batch_size, collate_fn=add2_t_dataset.collate_fn,\n",
    "                                  shuffle=True)"
   ],
   "metadata": {
    "id": "sxi8-5ckseEe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = StudentModel(params).to(params.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=StudDataset.encode_class(\"<PAD>\"))\n",
    "\n",
    "print(\"Initiated\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_valid_f1 = -float('inf')\n",
    "losses = {\"train\": [], \"val\": []}\n",
    "f1ns = {\"train\": [], \"val\": []}\n",
    "accs = {\"train\": [], \"val\": []}\n",
    "last_additional_train_loss = float('inf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = params.epochs\n",
    "epochs = epochs\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"EPOCH {}/{}\".format(str(epoch + 1),epochs))\n",
    "    if params.additional_dataset and last_additional_train_loss > 1.0  :\n",
    "        last_additional_train_loss = train(model, add2_t_dataloader, optimizer, criterion, params.device, conll=True)\n",
    "        print(f'\\t Train Loss CoNLL: {last_additional_train_loss:.3f}')\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, params.device)\n",
    "    print(f'\\t Train Loss main Dataset: {train_loss:.3f}')\n",
    "\n",
    "\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, dev_dataloader, criterion, params.device, dev_l)\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val Acc: {valid_acc:.2f}% | Val F1: {valid_f1 * 100:.2f}%')\n",
    "\n",
    "    if valid_f1 > best_valid_f1:\n",
    "        best_valid_f1 = valid_f1\n",
    "        torch.save(model.state_dict(), os.path.join(temp_path, 'model.ckpt'))\n",
    "        print(\"     NEW BEST MODEL\")\n",
    "\n",
    "    losses[\"train\"].append(train_loss)\n",
    "    losses[\"val\"].append(valid_loss)\n",
    "    f1ns[\"val\"].append(valid_f1)\n",
    "    accs[\"val\"].append(valid_acc)\n",
    "    print(\"     --- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "id": "eLCHLZTRpiti",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEPOCH \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mstr\u001B[39m(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m),epochs))\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m params\u001B[38;5;241m.\u001B[39madditional_dataset \u001B[38;5;129;01mand\u001B[39;00m last_additional_train_loss \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m  :\n\u001B[1;32m----> 8\u001B[0m     last_additional_train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd2_t_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconll\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m Train Loss CoNLL: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlast_additional_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m train(model, train_dataloader, optimizer, criterion, params\u001B[38;5;241m.\u001B[39mdevice)\n",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, optimizer, criterion, device, conll)\u001B[0m\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     14\u001B[0m hidden, out \u001B[38;5;241m=\u001B[39m model(x, conll\u001B[38;5;241m=\u001B[39mconll)\n\u001B[1;32m---> 16\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mner_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconll\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconll\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36mStudentModel.ner_loss\u001B[1;34m(self, y_pred, y, mask, criterion, conll)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mcrf:\n\u001B[0;32m     54\u001B[0m \t\u001B[38;5;28;01mif\u001B[39;00m conll:\n\u001B[1;32m---> 55\u001B[0m \t\tloss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconll_crf\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     56\u001B[0m \t\u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m \t\tloss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcrf(y_pred, y, mask))\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp2022-hw1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1047\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp2022-homeworks\\nlp2022-hw1-main\\hw1\\stud\\utility.py:387\u001B[0m, in \u001B[0;36mCRF.forward\u001B[1;34m(self, h, labels, mask)\u001B[0m\n\u001B[0;32m    377\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;124;03m\t\t:param h: hidden matrix (batch_size, seq_len, num_labels)\u001B[39;00m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;124;03m\t\t:param labels: answer labels of each sequence\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;124;03m\t\t:return: The log-likelihood (batch_size)\u001B[39;00m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;124;03m\t\t\"\"\"\u001B[39;00m\n\u001B[0;32m    386\u001B[0m         log_numerator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_numerator_log_likelihood(h, labels, mask)\n\u001B[1;32m--> 387\u001B[0m         log_denominator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_denominator_log_likelihood\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    389\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m log_numerator \u001B[38;5;241m-\u001B[39m log_denominator\n",
      "File \u001B[1;32m~\\PycharmProjects\\nlp2022-homeworks\\nlp2022-hw1-main\\hw1\\stud\\utility.py:497\u001B[0m, in \u001B[0;36mCRF._compute_denominator_log_likelihood\u001B[1;34m(self, h, mask)\u001B[0m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;66;03m# calculate t-th scores in each sequence\u001B[39;00m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;66;03m# (batch_size, num_labels)\u001B[39;00m\n\u001B[0;32m    496\u001B[0m score_t \u001B[38;5;241m=\u001B[39m before_score \u001B[38;5;241m+\u001B[39m h_t \u001B[38;5;241m+\u001B[39m trans\n\u001B[1;32m--> 497\u001B[0m score_t \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogsumexp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscore_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;66;03m# update scores\u001B[39;00m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;66;03m# (batch_size, num_labels)\u001B[39;00m\n\u001B[0;32m    501\u001B[0m score \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(mask_t, score_t, score)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot train graph"
   ],
   "metadata": {
    "id": "ukKiTN7Uhj-_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(losses[\"train\"], label=\"train\")\n",
    "plt.plot(losses[\"val\"], label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "uAQWefknhF0O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(accs[\"val\"], label=\"acc\")\n",
    "plt.plot(f1ns[\"val\"], label=\"f1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "QXunAlp3hF0O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "predictions_s = model.predict(dev_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(seqeval.metrics.classification_report(dev_l, predictions_s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "flat_label = [ x  for sentence in dev_l for x in sentence]\n",
    "flat_predict = [ x  for sentence in predictions_s for x in sentence]\n",
    "labels = [k for k,v in StudDataset.get_class_labels().items() if k != \"<PAD>\"]\n",
    "cf_matrix = confusion_matrix(flat_label, flat_predict, labels=labels)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palette = [\"#ffffff\", \"#e6f6ff\", \"#b8d5e6\", \"#8fb7cc\", \"#73a4bf\", \"#4d7e99\", \"#356a85\",\"#2d6480\" , \"#185a7a\", \"#004c71\"]\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True,\n",
    "            fmt='.2%',\n",
    "            cmap=sns.color_palette(palette, 9)\n",
    "            )\n",
    "\n",
    "ax.set_title('Confusion Matrix\\n\\n')\n",
    "ax.set_xlabel('\\nPredicted NER')\n",
    "ax.set_ylabel('Actual NER')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}